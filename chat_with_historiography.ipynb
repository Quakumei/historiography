{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76531a9-5e96-41eb-be93-16805b21a7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-06 16:40:41--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73,28K  --.-KB/s    in 0,1s    \n",
      "\n",
      "2024-06-06 16:40:42 (718 KB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8688db72-6d3d-4c51-b88f-ba3ce312afb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 16:50:49.355679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "# ollama\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# data = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n",
    "\n",
    "def create_docs(dataset_csv):\n",
    "    df = pd.read_csv(dataset_csv)\n",
    "    docs = []\n",
    "    for title, link, authors, year, full_text in df[['title','link','authors','year','full_text']].values.tolist():\n",
    "        doc = Document(text=full_text, extra_info={\"title\": title, \"link\": link, \"authors\": authors, \"year\": year})\n",
    "        docs.append(doc)\n",
    "    return docs    \n",
    "\n",
    "DATASET_CSV = 'data/historiography1_full.csv'\n",
    "docs = create_docs(DATASET_CSV)\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60eec1df-2685-4326-ab42-77ca8eb24837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /tmp/llama_index/models/llama-2-13b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    87.89 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  7412.96 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3520\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  2750.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2750.00 MiB, K (f16): 1375.00 MiB, V (f16): 1375.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   321.88 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.88 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_K_M.gguf\"\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3500,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 999},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "Settings.llm = llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac06eb9-8cab-4d76-b34f-fcaf6221de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = ChatMemoryBuffer.from_defaults(token_limit=512)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    # memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about a historic papers on \\\"nogai horde\\\". Please response in an academic manner.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48124247-19ad-44b1-9a63-e20442fa398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     274.54 ms\n",
      "llama_print_timings:      sample time =      50.87 ms /    93 runs   (    0.55 ms per token,  1828.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.40 ms /   115 tokens (    0.99 ms per token,  1005.28 tokens per second)\n",
      "llama_print_timings:        eval time =    1456.38 ms /    92 runs   (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:       total time =    1818.91 ms /   207 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt  : What's nogai horde? Give a short definition.\n",
      "Source:\n",
      "\ttitle: Ногайская Орда в системе международных отношений рубежа XV-XVI вв\n",
      "\tlink: https://cyberleninka.ru/article/n/nogayskaya-orda-v-sisteme-mezhdunarodnyh-otnosheniy-rubezha-xv-xvi-vv\n",
      "\tauthors: Моисеев Максим Владимирович\n",
      "\tyear: 2016\n",
      "Response:   The Nogai Horde was a medieval Turkic state that existed from the 14th to 16th centuries in the western part of the Eurasian steppes, primarily in present-day Russia and Kazakhstan. It was formed by the Nogai people, a branch of the Golden Horde, and was known for its military prowess and ability to maintain independence despite encroachment from neighboring powers.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's nogai horde? Give a short definition.\"\n",
    "response = chat_engine.chat(query)\n",
    "\n",
    "print(f\"Prompt  : {query}\")\n",
    "def get_reference(source_content):\n",
    "    return '\\n\\t'.join([source_content_line.strip() for source_content_line in source_content.split('\\n') if source_content_line.startswith((\"title:\", \"link:\", \"authors:\", \"year:\")) ])\n",
    "print(f\"Source:\\n\\t{get_reference(response.sources[0].content)}\")\n",
    "print(f\"Response: {response.response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2bd1457-429a-46bf-9e2a-fd312d34caed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     274.54 ms\n",
      "llama_print_timings:      sample time =      75.75 ms /   140 runs   (    0.54 ms per token,  1848.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1440.88 ms /  2769 tokens (    0.52 ms per token,  1921.75 tokens per second)\n",
      "llama_print_timings:        eval time =    2343.63 ms /   139 runs   (   16.86 ms per token,    59.31 tokens per second)\n",
      "llama_print_timings:       total time =    4166.48 ms /  2908 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt  : What's Sarai? Give a short definition.\n",
      "Source:\n",
      "\ttitle: О книге Б. Г. Аягана \"Абулхаир Шейбанид - последний правитель Дашти-Кыпчака\"\n",
      "\tlink: https://cyberleninka.ru/article/n/o-knige-b-g-ayagana-abulhair-sheybanid-posledniy-pravitel-dashti-kypchaka\n",
      "\tauthors: Алпысбес Махсат Алпысбесулы\n",
      "\tyear: 2019\n",
      "\ttitle: Специфика вотчинных прав башкир-семиродцев\n",
      "\tlink: https://cyberleninka.ru/article/n/spetsifika-votchinnyh-prav-bashkir-semirodtsev\n",
      "\tauthors: Азнабаев Б. А.\n",
      "\tyear: 2012\n",
      "Response:   Sarai (also spelled Saray or Sarai-Jük) is a medieval city located in present-day Kazakhstan, near the Caspian Sea. It was the capital of the Golden Horde, a Mongol khanate that ruled over much of Eastern Europe and Central Asia during the 13th to 14th centuries. Sarai was an important center of trade, culture, and religion, and was known for its impressive architecture, including mosques, palaces, and bathhouses. Today, the site of Sarai is a UNESCO World Heritage Site and is being excavated and studied by archaeologists.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's Sarai? Give a short definition.\"\n",
    "response = chat_engine.chat(query)\n",
    "\n",
    "print(f\"Prompt  : {query}\")\n",
    "def get_reference(source_content):\n",
    "    return '\\n\\t'.join([source_content_line.strip() for source_content_line in source_content.split('\\n') if source_content_line.startswith((\"title:\", \"link:\", \"authors:\", \"year:\")) ])\n",
    "print(f\"Source:\\n\\t{get_reference(response.sources[0].content)}\")\n",
    "print(f\"Response: {response.response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fefc39-a15c-4b97-9c3e-aee2feedb931",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's ? Give a short definition.\"\n",
    "response = chat_engine.chat(query)\n",
    "\n",
    "print(f\"Prompt  : {query}\")\n",
    "def get_reference(source_content):\n",
    "    return '\\n\\t'.join([source_content_line.strip() for source_content_line in source_content.split('\\n') if source_content_line.startswith((\"title:\", \"link:\", \"authors:\", \"year:\")) ])\n",
    "print(f\"Source:\\n\\t{get_reference(response.sources[0].content)}\")\n",
    "print(f\"Response: {response.response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
